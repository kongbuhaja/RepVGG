{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hs/anaconda3/envs/tf28/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGGLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_channel, out_channel, stride, padding, export=False):\n",
    "        super().__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.export = export\n",
    "\n",
    "        if not self.export:\n",
    "            self.zero_pad3x3 = tf.keras.layers.ZeroPadding2D(padding=self.padding)\n",
    "            self.conv3x3 = tf.keras.layers.Conv2D(self.out_channel, 3, self.stride, padding='valid', use_bias=False)\n",
    "            self.bn3x3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "            self.zero_pad1x1 = tf.keras.layers.ZeroPadding2D(padding=self.padding - 1)\n",
    "            self.conv1x1 = tf.keras.layers.Conv2D(self.out_channel, 1, self.stride, padding='valid', use_bias=False)\n",
    "            self.bn1x1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "            self.bnid = tf.keras.layers.BatchNormalization() if self.stride == 1 and self.in_channel == self.out_channel else None\n",
    "        else:\n",
    "            self.reparam_layer = tf.keras.Sequential([\n",
    "                tf.keras.layers.ZeroPadding2D(padding=self.padding),\n",
    "                tf.keras.layers.Conv2D(self.out_channel, 3, self.stride, padding='valid', use_bias=True)\n",
    "                ])\n",
    " \n",
    "        self.activation = tf.keras.layers.LeakyReLU()\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        if not self.export:\n",
    "            input3x3 = self.zero_pad3x3(input)\n",
    "            input1x1 = self.zero_pad1x1(input)\n",
    "            id_out = 0 if self.bnid is None else self.bnid(input, training)\n",
    "            x = self.bn3x3(self.conv3x3(input3x3), training) + self.bn1x1(self.conv1x1(input1x1), training) + id_out\n",
    "        else:\n",
    "            x = self.reparam_layer(input)\n",
    "            \n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    def get_reparam(self):\n",
    "        kernel3x3, bias3x3 = self.reparam_branch(self.conv3x3, self.bn3x3)\n",
    "        kernel1x1, bias1x1 = self.reparam_branch(self.conv1x1, self.bn1x1)\n",
    "        kernelid, biasid = self.reparam_branch(None, self.bnid)\n",
    "        return [kernel3x3 + self.pad_1x1_to_3x3(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid]\n",
    "    \n",
    "    def set_reparam(self, param):\n",
    "        self.reparam_layer.layers[1].set_weights(param)\n",
    "\n",
    "    def reparam_branch(self, conv, bn):\n",
    "        if conv is None:\n",
    "            if bn is None:\n",
    "                return 0, 0\n",
    "            kernel = np.zeros([3, 3, self.in_channel, self.in_channel], np.float32)\n",
    "            for i in range(self.in_channel):\n",
    "                kernel[1, 1, i, i] = 1\n",
    "        else:\n",
    "            kernel = conv.weights[0]\n",
    "\n",
    "        std = tf.sqrt(bn.moving_variance + bn.epsilon)\n",
    "    \n",
    "        return kernel * bn.gamma / std, bn.beta - bn.moving_mean * bn.gamma / std\n",
    "\n",
    "    def pad_1x1_to_3x3(self, kernel1x1):\n",
    "        return tf.pad(kernel1x1, tf.constant([[1,1], [1,1], [0,0], [0,0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGGBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_channel, out_channel, stride, num_block, export=False):\n",
    "        super().__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.stride = stride\n",
    "        self.num_block = num_block\n",
    "        self.export = export\n",
    "\n",
    "        self.rep_vgg_block = [RepVGGLayer(self.in_channel, self.out_channel, self.stride, 1, self.export)]\n",
    "\n",
    "        for l in range(self.num_block-1):\n",
    "            self.rep_vgg_block += [RepVGGLayer(self.out_channel, self.out_channel, 1, 1, self.export)]\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        x = input\n",
    "        for l in range(self.num_block):\n",
    "            x = self.rep_vgg_block[l](x, training)\n",
    "        return x\n",
    "\n",
    "    def get_reparam(self):\n",
    "        param = []\n",
    "        for l in range(self.num_block):\n",
    "            param += [self.rep_vgg_block[l].get_reparam()]\n",
    "\n",
    "        return param\n",
    "    \n",
    "    def set_reparam(self, param):\n",
    "        for l in range(self.num_block):\n",
    "            self.rep_vgg_block[l].set_reparam(param[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGG(tf.keras.Model):\n",
    "    def __init__(self, num_blocks, num_classes, export=False):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_classes = num_classes\n",
    "        self.export = export\n",
    "\n",
    "        self.stage = [\n",
    "            RepVGGBlock(1, 64, 2, self.num_blocks[0], self.export),\n",
    "            RepVGGBlock(64, 64, 2, self.num_blocks[1], self.export),\n",
    "            RepVGGBlock(64, 128, 2, self.num_blocks[2], self.export), \n",
    "            RepVGGBlock(128, 256, 2, self.num_blocks[3], self.export),\n",
    "            RepVGGBlock(256, 512, 2, self.num_blocks[4], self.export)]\n",
    "        self.gap = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.linear = tf.keras.layers.Dense(self.num_classes)\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        x = input\n",
    "        for l in range(len(self.stage)):\n",
    "            x = self.stage[l](x, training)\n",
    "        x = self.gap(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def get_reparam(self):\n",
    "        param = []\n",
    "        for l in range(len(self.stage)):\n",
    "            param += [self.stage[l].get_reparam()]\n",
    "        param += [self.linear.weights]\n",
    "        return param\n",
    "    \n",
    "    def set_reparam(self, param):\n",
    "        for l in range(len(self.stage)):\n",
    "            self.stage[l].set_reparam(param[l])\n",
    "        self.linear.set_weights(param[-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 21:05:32.570042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-23 21:05:32.588688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-23 21:05:32.588863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-23 21:05:32.589354: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-23 21:05:32.589961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-23 21:05:32.590125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-23 21:05:32.590269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-23 21:05:32.878856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-23 21:05:32.878991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-23 21:05:32.879094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-23 21:05:32.879191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18765 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0e:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = RepVGG([1, 2, 4, 14, 1], num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 512\n",
    "\n",
    "data, info = tfds.load('mnist', with_info=True, as_supervised=True, data_dir='./data')\n",
    "train_data = data['train']\n",
    "test_data = data['test']\n",
    "\n",
    "train_data = train_data.batch(batch_size, drop_remainder=True, num_parallel_calls=-1).cache().shuffle(60000).prefetch(-1)\n",
    "test_data = test_data.batch(batch_size, drop_remainder=True, num_parallel_calls=-1).cache().prefetch(-1)\n",
    "\n",
    "train_dataset_length = info.splits['train'].num_examples // batch_size\n",
    "test_dataset_length = info.splits['test'].num_examples // batch_size\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "max_loss = 1e+30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 1/10:   0%|\u001b[31m          \u001b[0m| 0/117 [00:00<?, ?it/s]2023-07-23 21:05:37.992916: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8900\n",
      "2023-07-23 21:05:38.444787: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "train epoch 1/10: 100%|\u001b[31m==========\u001b[0m| 117/117 [00:11<00:00, 10.05it/s, lr=0.0010, loss=0.81892]\n",
      "train epoch 2/10: 100%|\u001b[31m==========\u001b[0m| 117/117 [00:09<00:00, 12.14it/s, lr=0.0010, loss=0.47193]\n",
      "train epoch 3/10: 100%|\u001b[31m==========\u001b[0m| 117/117 [00:09<00:00, 11.93it/s, lr=0.0010, loss=0.35603]\n",
      "train epoch 4/10: 100%|\u001b[31m==========\u001b[0m| 117/117 [00:09<00:00, 12.00it/s, lr=0.0010, loss=0.28830]\n",
      "train epoch 5/10: 100%|\u001b[31m==========\u001b[0m| 117/117 [00:09<00:00, 11.92it/s, lr=0.0010, loss=0.24256]\n",
      "train epoch 6/10: 100%|\u001b[31m==========\u001b[0m| 117/117 [00:09<00:00, 12.16it/s, lr=0.0010, loss=0.20940]\n",
      "train epoch 7/10: 100%|\u001b[31m==========\u001b[0m| 117/117 [00:09<00:00, 12.00it/s, lr=0.0010, loss=0.18370]\n",
      "train epoch 8/10: 100%|\u001b[31m==========\u001b[0m| 117/117 [00:09<00:00, 12.03it/s, lr=0.0010, loss=0.16323]\n",
      "train epoch 9/10: 100%|\u001b[31m==========\u001b[0m| 117/117 [00:09<00:00, 12.09it/s, lr=0.0010, loss=0.14633]\n",
      "train epoch 10/10: 100%|\u001b[31m==========\u001b[0m| 117/117 [00:09<00:00, 12.07it/s, lr=0.0010, loss=0.13216]\n"
     ]
    }
   ],
   "source": [
    "def train(model):\n",
    "    for epoch in range(epochs):\n",
    "        train_tqdm = tqdm.tqdm(train_data, total=train_dataset_length, desc=f'train epoch {epoch+1}/{epochs}', ascii=' =', colour='red')\n",
    "        total_loss = 0\n",
    "        for iter, batch_data in enumerate(train_tqdm):\n",
    "            if iter >= 10:\n",
    "                optimizer.lr.assign(1e-3)\n",
    "            elif iter >= 70:\n",
    "                optimizer.lr.assign(1e-4)\n",
    "            x = tf.cast(batch_data[0], tf.float32)/255\n",
    "            y = tf.cast(batch_data[1], tf.float32)\n",
    "            with tf.GradientTape() as train_tape:\n",
    "                preds = model(x, True)\n",
    "                loss_ = loss(y, preds)\n",
    "                gradients = train_tape.gradient(loss_, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            total_loss += loss_\n",
    "            loss_ = total_loss / (iter+1)\n",
    "            train_tqdm.set_postfix_str(f'lr={optimizer.lr.numpy():.4f}, loss={loss_.numpy():.5f}')\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_model = RepVGG([1, 2, 4, 14, 1], num_classes=10, export=True)\n",
    "rep_model.build((None, 28, 28, 1))\n",
    "rep_model.set_reparam(model.get_reparam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test : 100%|\u001b[34m==========\u001b[0m| 19/19 [00:00<00:00, 21.33it/s, [origin| correct: 9146/9728, time: 0.028/0.564] [reparam| correct: 9147/9728, time: 0.013/0.266] [error| 303.9244689941406]] \n"
     ]
    }
   ],
   "source": [
    "def test(model1, model2):\n",
    "    correct = 0\n",
    "    correct_re = 0\n",
    "    total = batch_size * test_dataset_length\n",
    "    total_time1 = 0\n",
    "    total_time2 = 0\n",
    "    error = 0\n",
    "\n",
    "    test_tqdm = tqdm.tqdm(test_data, total=test_dataset_length, desc=f'test ', ascii=' =', colour='blue')\n",
    "    for iter, batch_data in enumerate(test_tqdm):\n",
    "        x = tf.cast(batch_data[0], tf.float32)/255\n",
    "        y = batch_data[1]\n",
    "        t1 = time.time()\n",
    "        preds = model1(x)\n",
    "        t2 = time.time()\n",
    "        total_time1 += (t2-t1)\n",
    "        preds_re = model2(x, False)\n",
    "        t3 = time.time()\n",
    "        total_time2 += (t3-t2)\n",
    "        correct += tf.reduce_sum(tf.cast(y==tf.argmax(preds, -1), tf.int32))\n",
    "        correct_re += tf.reduce_sum(tf.cast(y==tf.argmax(preds_re, -1), tf.int32))\n",
    "        error += tf.reduce_sum(tf.abs(preds - preds_re))\n",
    "        test_tqdm.set_postfix_str(f'[origin| correct: {correct}/{total}, time: {t2-t1:.3f}/{total_time1:.3f}] [reparam| correct: {correct_re}/{total}, time: {t3-t2:.3f}/{total_time2:.3f}] [error| {error}]')\n",
    "test(model, rep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.416607e-05, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((batch_size, 28, 28, 1))\n",
    "y = model(x)\n",
    "rep_y = rep_model(x)\n",
    "print(tf.reduce_mean(tf.math.square(y - rep_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf28",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
